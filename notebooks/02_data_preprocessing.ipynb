{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc35636",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1b8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "# import packages\n",
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89cf599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/rdoume/beerreviews?dataset_version_number=1&file_name=beer_reviews.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27.4M/27.4M [00:00<00:00, 68.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip of beer_reviews.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of data\n",
    "path = kagglehub.dataset_download(\"rdoume/beerreviews\", path='beer_reviews.csv', force_download = True)\n",
    "beer = pd.read_csv(path)\n",
    "#remove nulls\n",
    "beer = beer[-beer.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cea469",
   "metadata": {},
   "source": [
    "#### Multiple reviews for the same item\n",
    "We found earlier that there were around 14000 instances of a user reviewing the same beer more than once. Since basic collaborative filtering frameworks only account for a single user-item interaction, we need to specify an approach for dealing with these cases. In our simple model, we'll take the most recent rating as the \"true\" value. Later we might experiment with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10fad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a new dataframe\n",
    "beer_simple = beer.copy()\n",
    "# sort by the relevant columns\n",
    "beer_simple = beer_simple.sort_values(by=['review_profilename', 'beer_beerid', 'review_time'])\n",
    "# keep only the most recent review for the user-beer key\n",
    "beer_simple = beer_simple.drop_duplicates(subset=['review_profilename', 'beer_beerid'], keep=\"last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2b7456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "review_profilename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "beer_beerid",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "389fc4cc-22b0-4941-8992-b01fd79f789f",
       "rows": [],
       "shape": {
        "columns": 2,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_profilename, beer_beerid]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test using SQL\n",
    "query = \"SELECT review_profilename, beer_beerid \\\n",
    "    FROM beer_simple GROUP BY review_profilename, beer_beerid\\\n",
    "    HAVING COUNT(*)>1 \\\n",
    "    ORDER BY review_profilename, beer_beerid\"\n",
    "#use duckdb to query the data\n",
    "db.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30a4d2",
   "metadata": {},
   "source": [
    "#### Threshold Choice\n",
    "We're going to look at the performance of models using several different thresholds for review counts. There are some different considerations to make. First of all, we saw from the EDA that many beers and users only have one review - this is the cold start problem. To construct a meaningful collaborative filter model, we'll need at least three reviews per user/item. In the special case of using 3 as a threshold, we'll have to forgo the validation set entirely so that we have multiple data points per user/item. We'll investigate how different thresholds affect the tradeoff between coverage of recommended items and the quality of recommendations.\n",
    "\n",
    "As a baseline, we're going to start with a requiring at least 5 reviews per user and 3 reviews per item. These thresholds have been chosen since we want to balance allowing the model to recommend a large amount of items (less strict item threshold) while providing high-quality recommendations (stricter user threshold). Later, we'll experiment with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62700372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for users and beers with the specific threshold\n",
    "baseline = beer_simple.copy()\n",
    "baseline = baseline.groupby('beer_beerid').filter(lambda x: x.shape[0] >= 3)\n",
    "baseline = baseline.groupby('review_profilename').filter(lambda x: x.shape[0] >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92fb232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9ab2d03e-d2ce-42c3-b021-21edf1b4cb9d",
       "rows": [
        [
         "review_profilename",
         "32908"
        ],
        [
         "beer_beerid",
         "49000"
        ],
        [
         "beer_style",
         "104"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "review_profilename    32908\n",
       "beer_beerid           49000\n",
       "beer_style              104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_simple.nunique().loc[['review_profilename','beer_beerid', 'beer_style']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34b7571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f4cfae4c-3b8f-43ca-90fa-27b0fa26b8bf",
       "rows": [
        [
         "review_profilename",
         "14556"
        ],
        [
         "beer_beerid",
         "26113"
        ],
        [
         "beer_style",
         "104"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "review_profilename    14556\n",
       "beer_beerid           26113\n",
       "beer_style              104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.nunique().loc[['review_profilename','beer_beerid', 'beer_style']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fa60e",
   "metadata": {},
   "source": [
    "In this case, we see that we've retained over half of our items. As our model is quite simple, we'll lose a lot of coverage (almost half of all items). To properly address this, we would need to expand our model (e.g. using content-based recommendations with NLP), but since this is a simple project, we'll proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66dae2",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "Now it's time to split our data. We're going to leave the last rating as a test - we'll try and predict a user's *next* rating using all their past ratings as training data. This data splitting method approximates many real-world use cases, where we might want to predict a user's future behaviour given their actions until the current time. First, we need to encode the users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf21ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: encode users and items to integer indices\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "# fit encoders to the values in the set\n",
    "user_encoder.fit(baseline['review_profilename'])  \n",
    "item_encoder.fit(baseline['beer_beerid'])\n",
    "# create a mapping from original values to integer indices\n",
    "user_map = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "item_map = dict(zip(item_encoder.classes_, item_encoder.transform(item_encoder.classes_)))\n",
    "# make mapped columns in validation set\n",
    "baseline.loc[:, 'user_idx'] = user_encoder.transform(baseline['review_profilename'])\n",
    "baseline.loc[:, 'item_idx'] = item_encoder.transform(baseline['beer_beerid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f9863b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# generate test set and update training set\n",
    "# save the last review for each user\n",
    "test = baseline.drop_duplicates(subset=['review_profilename'], keep=\"last\")\n",
    "# remove last review in dataframe\n",
    "train = baseline.groupby('review_profilename', group_keys=False).apply(\n",
    "    lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73328f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# generate validation set and update training set\n",
    "# save the last review for each user\n",
    "validation = train.drop_duplicates(subset=['review_profilename'], keep=\"last\")\n",
    "# remove last review in dataframe\n",
    "train = train.groupby('review_profilename', group_keys=False).apply(\n",
    "    lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "533cf793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that we've split correctly\n",
    "baseline.shape[0] == train.shape[0] + validation.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbb0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only relevant columns\n",
    "cols = ['review_profilename','beer_beerid', 'review_overall', 'user_idx', 'item_idx']\n",
    "train = train[cols]\n",
    "validation = validation[cols]\n",
    "test = test[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23eb21d",
   "metadata": {},
   "source": [
    "#### Formatting our Data for CF\n",
    "Now we need to make a user-item matrix. Our simple model is only going to use the overall rating data. We will filter out the unseen items in the validation and test sets as CF is incapable of making meaningful predictions on unseen items. We'll add these items back after we choose a model and train it on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd3f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save known items\n",
    "known_items = set(train['item_idx'])\n",
    "# remove unknown items from validation\n",
    "validation = validation[validation['item_idx'].isin(known_items)].copy()\n",
    "# remove unknown items from test\n",
    "test = test[test['item_idx'].isin(known_items)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715b7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(data, num_users, num_items):\n",
    "    # create sparse matrix\n",
    "    ratings = data['review_overall'].values\n",
    "    rows = data['user_idx']\n",
    "    cols = data['item_idx']\n",
    "    coo = coo_matrix((ratings, (rows, cols)), shape=(num_users, num_items))\n",
    "    return coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "911e78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse matrix\n",
    "n_users = train['user_idx'].max() + 1\n",
    "n_items = train['item_idx'].max() + 1\n",
    "sparse = create_sparse_matrix(train, n_users, n_items)\n",
    "# convert to csr for efficient row ops\n",
    "ui_csr = sparse.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c628dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save matrices for use in other files\n",
    "from scipy.sparse import save_npz\n",
    "save_npz(\"../data/ui_csr.npz\", ui_csr)\n",
    "\n",
    "data = {\n",
    "    \"train\": train,\n",
    "    \"validation\": validation,\n",
    "    \"test\": test,\n",
    "    \"baseline\": baseline\n",
    "}\n",
    "\n",
    "with open(\"../data/dataframes.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb8cf4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle encoders\n",
    "with open(\"../artifacts/user_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_encoder, f)\n",
    "\n",
    "with open(\"../artifacts/item_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(item_encoder, f)\n",
    "\n",
    "with open(\"../artifacts/user_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_map, f)\n",
    "\n",
    "with open(\"../artifacts/item_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(item_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
