{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf333602",
   "metadata": {},
   "source": [
    "# Simple Neighbourhood Approach (manually implementing user-based CF)\n",
    "As a first step, we will use basic neighbourhood-based collaborative filtering (CF) techniques, with a simple model as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de69b10",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "29db0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import powerlaw as pl\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb as db\n",
    "import recbole as rb\n",
    "import scipy as sp\n",
    "import surprise as sks\n",
    "import sklearn as sk\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7c6d5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27.4M/27.4M [00:00<00:00, 84.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of data\n",
    "path = kagglehub.dataset_download(\"rdoume/beerreviews\", path='beer_reviews.csv', force_download = True)\n",
    "beer = pd.read_csv(path)\n",
    "#remove nulls\n",
    "beer = beer[-beer.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac37d1e",
   "metadata": {},
   "source": [
    "#### Multiple reviews for the same item\n",
    "We found earlier that there were around 14000 instances of a user reviewing the same beer more than once. Since basic collaborative filtering frameworks only account for a single user-item interaction, we need to specify an approach for dealing with these cases. In our simple model, we'll take the most recent rating as the \"true\" value. Later we might experiment with different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "19f26916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a new dataframe\n",
    "beer_simple = beer.copy()\n",
    "# sort by the relevant columns\n",
    "beer_simple = beer_simple.sort_values(by=['review_profilename', 'beer_beerid', 'review_time'])\n",
    "# keep only the most recent review for the user-beer key\n",
    "beer_simple = beer_simple.drop_duplicates(subset=['review_profilename', 'beer_beerid'], keep=\"last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f7799a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_profilename</th>\n",
       "      <th>beer_beerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_profilename, beer_beerid]\n",
       "Index: []"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test using SQL\n",
    "query = \"SELECT review_profilename, beer_beerid \\\n",
    "    FROM beer_simple GROUP BY review_profilename, beer_beerid\\\n",
    "    HAVING COUNT(*)>1 \\\n",
    "    ORDER BY review_profilename, beer_beerid\"\n",
    "#use duckdb to query the data\n",
    "db.sql(query).df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc00d85",
   "metadata": {},
   "source": [
    "#### Threshold Choice\n",
    "We're going to look at the performance of models using several different thresholds for review counts. There are some different considerations to make. First of all, we saw from the EDA that many beers and users only have one review - this is the cold start problem. To construct a meaningful collaborative filter model, we'll need at least three reviews per user/item. In the special case of using 3 as a threshold, we'll have to forgo the validation set entirely so that we have multiple data points per user/item. We'll investigate how different thresholds affect the tradeoff between coverage of recommended items and the quality of recommendations.\n",
    "\n",
    "As a baseline, we're going to start with a requiring at least 5 reviews per user and 3 reviews per item. These thresholds have been chosen since we want to balance allowing the model to recommend a large amount of items (less strict item threshold) while providing high-quality recommendations (stricter user threshold). Later, we'll experiment with different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "767ad0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for users and beers with the specific threshold\n",
    "# keep only relevant columns\n",
    "baseline = beer_simple[['review_profilename', 'beer_beerid', 'review_time']]\n",
    "baseline = beer_simple.groupby('review_profilename').filter(lambda x: x.shape[0] >= 5)\n",
    "baseline = baseline.groupby('beer_beerid').filter(lambda x: x.shape[0] >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "3c7e3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_profilename    32908\n",
       "beer_beerid           49000\n",
       "beer_style              104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_simple.nunique().loc[['review_profilename','beer_beerid', 'beer_style']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "366573d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_profilename    14602\n",
       "beer_beerid           25851\n",
       "beer_style              104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.nunique().loc[['review_profilename','beer_beerid', 'beer_style']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03afc1",
   "metadata": {},
   "source": [
    "In this case, we see that we've retained around 52% of items. As our model is quite simple, we'll lose a lot of coverage (almost half of all items). To properly address this, we would need to expand our model (e.g. using content-based recommendations with NLP), but since this is a simple project, we'll proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836e0ea",
   "metadata": {},
   "source": [
    "#### Data Splitting\n",
    "Now it's time to split our data. We're going to leave the last rating as a test - we'll try and predict a user's *next* rating using all their past ratings as training data. This data splitting method approximates many real-world use cases, where we might want to predict a user's future behaviour given their actions until the current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "7332714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# generate test set and update training set\n",
    "# save the last review for each user\n",
    "test = baseline.drop_duplicates(subset=['review_profilename'], keep=\"last\")\n",
    "# remove last review in dataframe\n",
    "train = baseline.groupby('review_profilename', group_keys=False).apply(\n",
    "    lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9996e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# generate validation set and update training set\n",
    "# save the last review for each user\n",
    "validation = train.drop_duplicates(subset=['review_profilename'], keep=\"last\")\n",
    "# remove last review in dataframe\n",
    "train = train.groupby('review_profilename', group_keys=False).apply(\n",
    "    lambda x: x.iloc[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e3b013cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test that we've split correctly\n",
    "baseline.shape[0] == train.shape[0] + validation.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf86562",
   "metadata": {},
   "source": [
    "#### Formatting our Data for CF\n",
    "Now we need to make a user-item matrix. Our simple model is only going to use the overall rating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "27f250e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: encode users and items to integer indices\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "# fit encoders to the values in the set\n",
    "user_encoder.fit(train['review_profilename'])  \n",
    "item_encoder.fit(train['beer_beerid'])\n",
    "# create a mapping from original values to integer indices\n",
    "user_map = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "item_map = dict(zip(item_encoder.classes_, item_encoder.transform(item_encoder.classes_)))\n",
    "# transform the user and item columns in the train set\n",
    "user_idx = user_encoder.fit_transform(train['review_profilename'])\n",
    "item_idx = item_encoder.fit_transform(train['beer_beerid'])\n",
    "\n",
    "# step 2: create the sparse matrix\n",
    "ratings = train['review_overall'].values\n",
    "coo = coo_matrix((ratings, (user_idx, item_idx)))\n",
    "#convert to CSR format for efficiency\n",
    "ui_csr = coo.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e84645e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for user and item maps\n",
    "user_map = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "item_map = dict(zip(item_encoder.classes_, item_encoder.transform(item_encoder.classes_)))\n",
    "\n",
    "validation['user_idx'] = validation['review_profilename'].map(user_map)\n",
    "validation['item_idx'] = validation['beer_beerid'].map(item_map)\n",
    "\n",
    "#drop missing values\n",
    "validation = validation.dropna(subset=['user_idx', 'item_idx']).astype({'user_idx': int, 'item_idx': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ce8fc",
   "metadata": {},
   "source": [
    "We're going to mean-centre each user's score to account for the fact that some users tend to be more lenient or harsh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1e70f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sum of scores per user\n",
    "scores = ui_csr.sum(axis=1).A1\n",
    "# count number of user reviews\n",
    "counts = np.diff(ui_csr.indptr)\n",
    "# get mean vector\n",
    "mean_scores = scores / counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c7a10",
   "metadata": {},
   "source": [
    "Now, it's time to compute the similar matrix. We'll use cosine similarity on our mean-centred data (adjusted-cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "637f564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(ui_csr, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1283a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 0 stored elements and shape (2, 1)>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_csr[[1,2],0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e76c0",
   "metadata": {},
   "source": [
    "### Training\n",
    "We're ready to begin training our model. For this simple example, we'll validate our choice of $k$ nearest neighbours, first defining a prediction function. Note that we're using a prediction function and rounding to the nearest .5 instead of approaching our ratings as a classificaiton problem. We use this approach for simplicity.\n",
    "#### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "beef3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user, item, train, similarity, mean_scores, k, clipped=True):\n",
    "    \"\"\"\n",
    "    Predict ratings for the user-item pair using k nearest neighbours, \n",
    "    rounded to the nearest .5 and capped in [0,5]. \n",
    "    \n",
    "    Parameters:\n",
    "    -user: user index for whom to predict ratings\n",
    "    -item: item index for which to predict ratings\n",
    "    -train: training data in sparse matrix format\n",
    "    -similarity: similarity matrix in sparse format\n",
    "    -mean_scores: mean scores for each user\n",
    "    -k: number of nearest neighbouts to consider\n",
    "    \n",
    "    Returns: ordinal prediction for user-item pair\n",
    "    \"\"\"\n",
    "\n",
    "    #find neighbours of user for item\n",
    "    nbs = train[:,item].nonzero()[0]\n",
    "    nbs = nbs[nbs != user] #exclude self\n",
    "    if nbs.size == 0:\n",
    "        # no neighbours, return mean score\n",
    "        return mean_scores[user]\n",
    "    \n",
    "    # get ratings and mean-centre them\n",
    "    ratings = train[nbs,item].toarray().flatten()\n",
    "    ratings -= mean_scores[nbs]\n",
    "    # set limit for k\n",
    "    k = min(k, nbs.size)\n",
    "\n",
    "    # get similarity scores\n",
    "    sims = similarity[user, :].toarray().flatten()\n",
    "    # get similarity scores for neighbours\n",
    "    sims = sims[nbs]\n",
    "    # take k-nearest similarities\n",
    "    sims = sims[np.argsort(sims)[-k:]]\n",
    "    # get corresponding k-nearest ratings\n",
    "    ratings = ratings[np.argsort(sims)[-k:]]\n",
    "    # compute weighted average\n",
    "    if np.sum(np.abs(sims)) == 0:\n",
    "        return mean_scores[user]\n",
    "    weighted_avg = np.dot(sims, ratings) / np.sum(np.abs(sims))\n",
    "    # recenter\n",
    "    weighted_avg += mean_scores[user]\n",
    "    if clipped == False:\n",
    "        return weighted_avg\n",
    "    # round to nearest .5\n",
    "    weighted_avg = np.round(weighted_avg * 2) / 2 \n",
    "    # clip to [0,5]\n",
    "    weighted_avg = np.clip(weighted_avg, 0, 5)\n",
    "    # return prediction\n",
    "    return weighted_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0101370",
   "metadata": {},
   "source": [
    "#### Evaluating the effect of neighbourhood size\n",
    "Now we'll evaluate over different choices of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6647422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_k(train, validation, similarity, mean_scores, k):\n",
    "    preds = []\n",
    "    actuals = []\n",
    "\n",
    "    for row in validation.itertuples(index=False):\n",
    "        u = row.user_idx\n",
    "        i = row.item_idx\n",
    "        true_r = row.review_overall\n",
    "        pred = predict(u, i, train, similarity, mean_scores, k)\n",
    "        preds.append(pred)\n",
    "        actuals.append(true_r)\n",
    "\n",
    "    return np.sqrt(mean_squared_error(actuals, preds))  # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2eb686f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE for user-based CF with 3-NN is           0.8292517772308085\n",
      "The RMSE for user-based CF with 5-NN is           0.7954208555961889\n",
      "The RMSE for user-based CF with 10-NN is           0.7741529402438432\n",
      "The RMSE for user-based CF with 20-NN is           0.7671234633811842\n",
      "The RMSE for user-based CF with 50-NN is           0.7615911347614778\n",
      "The RMSE for user-based CF with 100-NN is           0.7591672808836532\n"
     ]
    }
   ],
   "source": [
    "# let's experiment with different values of k\n",
    "k_values = [3, 5, 10, 20, 50, 100]\n",
    "train, validation, similarity, mean_scores = ui_csr, validation, cos_sim, mean_scores\n",
    "for k in k_values:\n",
    "    print(f'The RMSE for user-based CF with {k}-NN is \\\n",
    "          {evaluate_k(train, validation, similarity, mean_scores, k)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3dfd3",
   "metadata": {},
   "source": [
    "#### Top-N predictions\n",
    "We're not only interested in prediction accuracy; we'd also like to know how effective our algorithm is at predicting novel or less popular items. To measure this, we'll look at the top-N items as calculated by taking the $N$ items for a user with the highest predicted ratings. To avoid interminable runtime, we'll use a a fast matrix-based function which precomputes the most similar neighbours for each user (not for a user-item pair).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ee58827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_N_fast(user, train, similarity, mean_scores, k=10, N=10):\n",
    "    \"\"\"\n",
    "    Fast top-N prediction using user-based CF with vectorized matrix ops.\n",
    "\n",
    "    Parameters:\n",
    "    - user: target user index\n",
    "    - train: CSR matrix of shape (n_users, n_items)\n",
    "    - similarity: (n_users, n_users) sparse matrix or dense array\n",
    "    - mean_scores: array of user mean ratings\n",
    "    - k: number of nearest neighbours\n",
    "    - N: number of top items to return\n",
    "\n",
    "    Returns:\n",
    "    - List of top-N item indices predicted for the user\n",
    "    \"\"\"\n",
    "    # 1. Get top-k most similar users to target user\n",
    "    user_sims = similarity[user, :].toarray().flatten()\n",
    "    topk_idx = np.argsort(user_sims)[-k:]\n",
    "    topk_sims = user_sims[topk_idx]  # shape: (k,)\n",
    "\n",
    "    # 2. Get their ratings and mean-center\n",
    "    ratings = train[topk_idx, :].toarray()  # shape: (k, n_items)\n",
    "    means = mean_scores[topk_idx][:, np.newaxis]\n",
    "    ratings_centered = ratings - means  # shape: (k, n_items)\n",
    "\n",
    "    # 3. Weighted sum of centered ratings\n",
    "    numerator = topk_sims @ ratings_centered  # shape: (n_items,)\n",
    "    denominator = np.sum(np.abs(topk_sims)) + 1e-8  # to avoid div by 0\n",
    "\n",
    "    preds = mean_scores[user] + numerator / denominator  # shape: (n_items,)\n",
    "\n",
    "    # 4. Mask out already rated items\n",
    "    rated_items = train[user, :].nonzero()[1]\n",
    "    preds[rated_items] = -np.inf  # exclude known ratings\n",
    "\n",
    "    # 5. Return top-N items\n",
    "    top_N_items = np.argsort(preds)[-N:][::-1]\n",
    "    return top_N_items.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9689d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k = 3 nearest neighbours and top-10 beers recommended:\n",
      "The number of recommended beers over all users is 6488\n",
      "This corresponds to 25.18%         of all beers in the training matrix.\n",
      "For k = 5 nearest neighbours and top-10 beers recommended:\n",
      "The number of recommended beers over all users is 6146\n",
      "This corresponds to 23.85%         of all beers in the training matrix.\n",
      "For k = 10 nearest neighbours and top-10 beers recommended:\n",
      "The number of recommended beers over all users is 5051\n",
      "This corresponds to 19.60%         of all beers in the training matrix.\n",
      "For k = 20 nearest neighbours and top-10 beers recommended:\n",
      "The number of recommended beers over all users is 3822\n",
      "This corresponds to 14.83%         of all beers in the training matrix.\n"
     ]
    }
   ],
   "source": [
    "k_values = [3, 5, 10, 20]\n",
    "N = 10\n",
    "\n",
    "for k in k_values:\n",
    "    recommended_beers = set()\n",
    "    for user in range(ui_csr.shape[0]):\n",
    "        preds = predict_top_N_fast(user, train, similarity, mean_scores, k, N)\n",
    "        # update the set of recommended beers\n",
    "        recommended_beers.update(preds)\n",
    "    print(f'For k = {k} nearest neighbours and top-{N} beers recommended:')\n",
    "    print(f'The number of recommended beers over all users is {len(recommended_beers)}')\n",
    "    print(f'This corresponds to {len(recommended_beers) / ui_csr.shape[1] * 100:.2f}% \\\n",
    "        of all beers in the training matrix.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
